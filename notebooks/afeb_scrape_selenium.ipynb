{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98def7-c779-4f62-a733-62049f82bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe393a-f451-49e4-94d4-ab579422e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename=\"../logs/scrape.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fc4cb-4536-4759-86c9-946dbac381ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv(\n",
    "    \"../data/intermediate/companies_filling_minimal.csv\", index_col=0\n",
    ")\n",
    "companies[\"most_recent_filling\"] = pd.to_datetime(companies[\"most_recent_filling\"])\n",
    "companies_more_2008 = companies[companies[\"most_recent_filling\"].dt.year > 2008]\n",
    "\n",
    "ciks = list(companies_more_2008[\"CIK\"])\n",
    "len(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c3664-d616-484d-a6b5-5b6c71aef36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Anselme F.E. Borgeaud (aborgeaud@gmail.com)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d70913-4f2e-4425-8328-6abbc9e281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup) -> datetime:\n",
    "    ps = soup.findAll(\"p\")\n",
    "    bs = soup.findAll(\"b\")\n",
    "    texts = [\n",
    "        p.get_text(strip=True).replace(u\"\\xa0\", u\" \")\n",
    "        for p in ps + bs\n",
    "        if \"fiscal year ended\" in p.text\n",
    "    ]\n",
    "    if len(texts) > 0:\n",
    "        match = re.findall(r\"ended .*[0-9]{4}\", texts[0])\n",
    "        if len(match) == 1:\n",
    "            date_str = match[0].replace(\"ended \", \"\")\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "                return date\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "\n",
    "def find_value(td_elem) -> float:\n",
    "    def is_number(s):\n",
    "        return len(re.findall(r\"[0-9]+\", s)) > 0\n",
    "\n",
    "    elem = row_elem.find_next_sibling(\"td\")\n",
    "    if elem is None:\n",
    "        return None\n",
    "    i = 0\n",
    "    while not is_number(elem.text) and i < 4:\n",
    "        elem = elem.find_next_sibling(\"td\")\n",
    "    if is_number(elem.text):\n",
    "        try:\n",
    "            return float(elem.text.replace(\",\", \"\"))\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def find_value_in_table(soup, key: str) -> float:\n",
    "    key_elem = None\n",
    "    key_text = key.strip().replace(\"\\n\", \"_\").replace(\" \", \"_\").lower()\n",
    "    for e in soup.findAll(\"td\"):\n",
    "        text = e.text.strip().replace(\"\\n\", \"_\").replace(\" \", \"_\").lower()\n",
    "        if key_text == text:\n",
    "            key_elem = e\n",
    "    if key_elem:\n",
    "        return find_value(key_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d31a-add4-404a-b312-33c113483b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [\"total_current_assets\", \"total_current_liabilities\"]\n",
    "\n",
    "\n",
    "def run(ciks: list):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(8)\n",
    "\n",
    "    report_urls = []\n",
    "    data_dict = defaultdict(list)\n",
    "    data_df = None\n",
    "\n",
    "    for i, cik in enumerate(tqdm(ciks)):\n",
    "        if (i > 0) and (i % 30 == 0):\n",
    "            timestamp = time.time_ns()\n",
    "            file_report = f\"../data/intermediate/report_urls{timestamp}.pickle\"\n",
    "            with open(file_report, \"wb\") as f:\n",
    "                pickle.dump(report_urls, f)\n",
    "            data_df = pd.DataFrame(data_dict)\n",
    "            data_df.to_csv(f\"../data/intermediate/scraped_financials{timestamp}.csv\")\n",
    "\n",
    "        url = f\"https://www.sec.gov/edgar/browse/?CIK={cik}\"\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except:\n",
    "            logging.info(f\"{url} Did not get page\")\n",
    "            continue\n",
    "        time.sleep(4)\n",
    "\n",
    "        try:\n",
    "            search_elem = driver.find_element_by_xpath(\n",
    "                '//input[@placeholder=\"Search table\"]'\n",
    "            )\n",
    "            search_elem.send_keys(\"10-K \")  # blank space to avoid 10-K/A (amendments)\n",
    "        except:\n",
    "            logging.info(f\"{url} Did not input 10-K\")\n",
    "            continue\n",
    "\n",
    "        link_elems = driver.find_elements_by_class_name(\"document-link\")\n",
    "        annual_report_elems = [e for e in link_elems if \"Annual report\" in e.text]\n",
    "        annual_report_pages = []\n",
    "\n",
    "        for annual_report_elem in annual_report_elems:\n",
    "            try:\n",
    "                resp = requests.get(\n",
    "                    annual_report_elem.get_property(\"href\"), headers=headers, timeout=5\n",
    "                )\n",
    "                annual_report_pages.append(resp)\n",
    "            except:\n",
    "                href_elem = a.get_property(\"href\")\n",
    "                logging.info(f\"{href_elem} Did not fetch report page\")\n",
    "                pass\n",
    "\n",
    "        report_urls.extend([a.get_property(\"href\") for a in annual_report_elems])\n",
    "\n",
    "        for i, page in enumerate(annual_report_pages):\n",
    "            soup = BeautifulSoup(page.content)\n",
    "\n",
    "            try:\n",
    "                date_str = driver.find_elements_by_xpath(\n",
    "                    '//a[@data-index=\"reportDate\"]'\n",
    "                )[i].text\n",
    "                date = datetime.fromisoformat(date_str)\n",
    "            except:\n",
    "                logging.info(f\"{page.url} Did not read date\")\n",
    "                continue\n",
    "\n",
    "            tmp_dict = None\n",
    "            try:\n",
    "                tmp_dict = dict()\n",
    "                for entry in entries:\n",
    "                    tmp_dict[entry] = find_value_in_table(soup, entry)\n",
    "            except:\n",
    "                logging.info(f\"{page.url} Did not read html\")\n",
    "                continue\n",
    "            if (tmp_dict.keys() - set(entries)) == set():\n",
    "                for entry, value in tmp_dict.items():\n",
    "                    data_dict[entry].append(value)\n",
    "                data_dict[\"CIK\"].append(cik)\n",
    "                data_dict[\"date_filled\"].append(date)\n",
    "                data_dict[\"url\"].append(page.url)\n",
    "\n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    data_df.to_csv(\"../data/intermediate/scraped_financials.csv\")\n",
    "\n",
    "    with open(\"../data/intermediate/report_urls.pickle\", \"wb\") as f:\n",
    "        pickle.dump(report_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d696b-97d7-405c-819d-01afa9c5a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ciks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion",
   "language": "python",
   "name": "propulsion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
