{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ef5a99-12c4-4dfe-a4d1-eb2b0313f2c5",
   "metadata": {},
   "source": [
    "# Financial reports scraping from EDGAR\n",
    "- Request financial reports for S&P500 companies.\n",
    "- Using companies CIK (central indexing key on EDGAR database).\n",
    "- REST API get requests (older reports).\n",
    "- Selenium for the dynamic webpages (newer reports, since ~2019?).\n",
    "- BeautifulSoup + regex to parse report entries (e.g., total current assets).\n",
    "- For use to build a dataset for analysis of time series of financial report entries, and NLP using text comments in financial reports (not all the text seems to be accessible using the EDGAR API, so scraping is needed).\n",
    "- Provides name and email (as User-Agent), and respects the 10 requests/second limit to query the EDGAR database\n",
    "- This consists of 10s of Gb of data. It takes time and might need multiple runs due to Timeout errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45169baf-1b0f-4bbc-8b28-af4864db335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8fe393a-f451-49e4-94d4-ab579422e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from urllib3.exceptions import ConnectTimeoutError, NewConnectionError, ConnectionError\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename=\"../logs/scrape.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048fc4cb-4536-4759-86c9-946dbac381ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = pd.read_csv(\n",
    "    \"../data/intermediate/companies_filling_minimal.csv\", index_col=0\n",
    ")\n",
    "companies[\"most_recent_filling\"] = pd.to_datetime(companies[\"most_recent_filling\"])\n",
    "companies_more_2008 = companies[companies[\"most_recent_filling\"].dt.year > 2008]\n",
    "\n",
    "ciks = list(companies_more_2008[\"CIK\"])\n",
    "len(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd8f1a3-51a6-4e24-8fe6-107722c41ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2075"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciks_2021 = companies.loc[companies[\"most_recent_filling\"].dt.year == 2021, \"CIK\"]\n",
    "len(ciks_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188a5128-1169-42b1-a18c-26b9b8718d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500 = pd.read_csv(\"../data/processed/sp500.csv\")\n",
    "ciks = sp500[\"CIK\"]\n",
    "len(ciks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2c3664-d616-484d-a6b5-5b6c71aef36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Anselme F.E. Borgeaud (aborgeaud@gmail.com)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d70913-4f2e-4425-8328-6abbc9e281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to work on a BeautifulSoup object\n",
    "\n",
    "\n",
    "def find_value(td_elem) -> float:\n",
    "    def is_number(s):\n",
    "        return len(re.findall(r\"[0-9]+\", s)) > 0\n",
    "\n",
    "    elem = td_elem.find_next_sibling(\"td\")\n",
    "    if elem is None:\n",
    "        return None\n",
    "    i = 0\n",
    "    while elem and not is_number(elem.text) and i < 4:\n",
    "        elem = elem.find_next_sibling(\"td\")\n",
    "    if elem is None:\n",
    "        return None\n",
    "    if is_number(elem.text):\n",
    "        try:\n",
    "            return float(elem.text.replace(\",\", \"\"))\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def find_value_in_table(soup, key: str) -> float:\n",
    "    key_elem = None\n",
    "    key_text = key.strip().replace(\"\\n\", \"_\").replace(\" \", \"_\").lower()\n",
    "    for e in soup.findAll(\"td\"):\n",
    "        text = e.text.strip().replace(\"\\n\", \"_\").replace(\" \", \"_\").lower()\n",
    "        matches = re.findall(key_text, text)\n",
    "        if len(matches) > 0:\n",
    "            key_elem = e\n",
    "            break\n",
    "    #         if key_text == text:\n",
    "    #             key_elem = e\n",
    "    if key_elem:\n",
    "        return find_value(key_elem)\n",
    "\n",
    "\n",
    "def find_value_in_table_txt(text: str, key: str) -> float:\n",
    "    key_text = key.strip().lower().replace(\" \", \"_\")\n",
    "    text_ = text.lower().replace(\" \", \"_\")\n",
    "    occurences = [m.start() for m in re.finditer(key_text, text_)]\n",
    "    if len(occurences) > 0:\n",
    "        occurence = occurences[0]\n",
    "        line = text_[occurence : occurence + 200].replace(\",\", \"\")\n",
    "        values = re.findall(r\"[0-9]+\", line)\n",
    "        if len(values) > 0:\n",
    "            value = values[0]\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                value = None\n",
    "\n",
    "\n",
    "def find_values_in_table(soup, keys: str) -> float:\n",
    "    key_texts = [k.strip().lower().replace(\" \", \"_\") for k in keys]\n",
    "    key_elems = {k: None for k in key_texts}\n",
    "    key_set = set(key_texts)\n",
    "    found_keys = set()\n",
    "    for e in soup.findAll(\"td\"):\n",
    "        text = e.text.strip().replace(\"\\n\", \"_\").replace(\" \", \"_\").lower()\n",
    "        for key_text in key_set - found_keys:\n",
    "            match = re.search(key_text, text)\n",
    "            if match:\n",
    "                key_elems[key_text] = e\n",
    "                found_keys.add(key_text)\n",
    "    return [find_value(key_elems[k]) if key_elems[k] else None for k in key_texts]\n",
    "\n",
    "\n",
    "def find_values_in_table_txt(text: str, keys: str) -> float:\n",
    "    key_text = key.strip().lower().replace(\" \", \"_\")\n",
    "    text_ = text.lower().replace(\" \", \"_\")\n",
    "    occurences = [m.start() for m in re.finditer(key_text, text_)]\n",
    "    if len(occurences) > 0:\n",
    "        occurence = occurences[0]\n",
    "        line = text_[occurence : occurence + 200].replace(\",\", \"\")\n",
    "        values = re.findall(r\"[0-9]+\", line)\n",
    "        if len(values) > 0:\n",
    "            value = values[0]\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5d438f-3fa7-4370-8349-2050cfd00e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to work on a Selenium webdriver object\n",
    "\n",
    "\n",
    "def expend_10k_button(driver):\n",
    "    elem_10k = None\n",
    "    for elem in driver.find_elements_by_class_name(\"expandCollapse\"):\n",
    "        parent = elem.find_element_by_xpath(\"..\")\n",
    "        if \"10-K\" in parent.text:\n",
    "            elem_10k = parent\n",
    "            break\n",
    "    if elem_10k:\n",
    "        elem_10k.click()\n",
    "\n",
    "\n",
    "def click_view_all_10k(driver):\n",
    "    elem_view10k = driver.find_element_by_xpath(\n",
    "        '//button[@data-group=\"annualOrQuarterlyReports\"]'\n",
    "    )\n",
    "    elem_view10k.click()\n",
    "\n",
    "\n",
    "def input_search_10k(driver):\n",
    "    search_elem = driver.find_element_by_xpath('//input[@placeholder=\"Search table\"]')\n",
    "    # blank space to avoid 10-K/A (amendments)\n",
    "    search_elem.send_keys(\"10-K \")\n",
    "\n",
    "\n",
    "def html_url_from_xbrl_viewer(annual_report_elem, driver):\n",
    "    annual_report_elem.click()\n",
    "    time.sleep(0.2)\n",
    "    if len(driver.window_handles) > 1:\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "    else:\n",
    "        return None\n",
    "    t = 1\n",
    "    time.sleep(1)\n",
    "    html_elem = None\n",
    "    url = None\n",
    "    while not html_elem and t < 10:\n",
    "        try:\n",
    "            menu_elem = driver.find_element_by_xpath('//a[@id=\"menu-dropdown-link\"]')\n",
    "            menu_elem.click()\n",
    "            time.sleep(0.2)\n",
    "            html_elem = driver.find_element_by_id(\"form-information-html\")\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            t += 1\n",
    "    if html_elem:\n",
    "        url = html_elem.get_attribute(\"href\")\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    return url\n",
    "\n",
    "\n",
    "def is_xbrl(url):\n",
    "    return r\"ix?doc=\" in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0436d31a-add4-404a-b312-33c113483b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [\n",
    "    \"total_current_assets\",\n",
    "    \"total_current_liabilities\",\n",
    "    \"long-term_debt\",\n",
    "    \"total_liabilities\",\n",
    "    \"total_equity\",\n",
    "    r\"total_stockholders.?.?equity\",\n",
    "    r\"earnings_per_share_attributable.*diluted\",\n",
    "]\n",
    "\n",
    "\n",
    "def run(ciks: list):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(8)\n",
    "\n",
    "    report_urls = []\n",
    "    report_pages = defaultdict(list)\n",
    "    data_dict = defaultdict(list)\n",
    "    data_df = None\n",
    "\n",
    "    previous_file_report = None\n",
    "    previous_file_df = None\n",
    "    for i, cik in enumerate(tqdm(ciks)):\n",
    "        if (i > 0) and (i % 15 == 0):\n",
    "            timestamp = time.time_ns()\n",
    "            file_report = f\"../data/intermediate/report_urls{timestamp}.pickle\"\n",
    "            file_page = f\"../data/intermediate/report_pages_{i//15}.pickle\"\n",
    "            file_df = f\"../data/intermediate/scraped_financials{timestamp}.csv\"\n",
    "\n",
    "            with open(file_report, \"wb\") as f:\n",
    "                pickle.dump(report_urls, f)\n",
    "            d = {k: v for k, v in report_pages.items()}\n",
    "            with open(file_page, \"wb\") as f:\n",
    "                pickle.dump(d, f)\n",
    "            data_df = pd.DataFrame(data_dict)\n",
    "            data_df.to_csv(file_df)\n",
    "\n",
    "            if previous_file_report:\n",
    "                os.remove(previous_file_report)\n",
    "                os.remove(previous_file_df)\n",
    "            previous_file_report = file_report\n",
    "            previous_file_df = file_df\n",
    "\n",
    "            del report_pages\n",
    "            report_pages = defaultdict(list)\n",
    "\n",
    "        url = f\"https://www.sec.gov/edgar/browse/?CIK={cik}\"\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except:\n",
    "            logging.info(f\"{url} Did not get page\")\n",
    "            continue\n",
    "\n",
    "        # TODO in while loop to decrease wait time\n",
    "        time.sleep(4)\n",
    "\n",
    "        try:\n",
    "            input_search_10k(driver)\n",
    "        except:\n",
    "            logging.info(f\"{url} Did not input 10-K\")\n",
    "            try:\n",
    "                expend_10k_button(driver)\n",
    "                time.sleep(1.5)\n",
    "                click_view_all_10k(driver)\n",
    "                time.sleep(1.5)\n",
    "                input_search_10k(driver)\n",
    "            except:\n",
    "                logging.info(f\"{url} Did not expend and input 10-K\")\n",
    "                continue\n",
    "\n",
    "        link_elems = driver.find_elements_by_class_name(\"document-link\")\n",
    "        annual_report_elems = [\n",
    "            e\n",
    "            for e in link_elems\n",
    "            if \"Annual report\" in e.text\n",
    "            and \"right\" in e.get_attribute(\"data-placement\")\n",
    "        ]\n",
    "        report_urls.extend([a.get_property(\"href\") for a in annual_report_elems])\n",
    "\n",
    "        annual_report_pages = []\n",
    "\n",
    "        for annual_report_elem in annual_report_elems:\n",
    "            #             try:\n",
    "            page_url = annual_report_elem.get_property(\"href\")\n",
    "            if is_xbrl(page_url):\n",
    "                page_url = html_url_from_xbrl_viewer(annual_report_elem, driver)\n",
    "            if page_url:\n",
    "                count = 0\n",
    "                page = None\n",
    "                while not page and count < 3:\n",
    "                    try:\n",
    "                        page = requests.get(page_url, headers=headers, timeout=10)\n",
    "                    except (\n",
    "                        TimeoutError,\n",
    "                        NewConnectionError,\n",
    "                        ConnectionError,\n",
    "                        ConnectTimeoutError,\n",
    "                    ):\n",
    "                        time.sleep(1)\n",
    "                        count += 1\n",
    "                if page:\n",
    "                    annual_report_pages.append(page)\n",
    "            else:\n",
    "                href_elem = annual_report_elem.get_property(\"href\")\n",
    "                logging.info(f\"{href_elem} Did not fetch report page\")\n",
    "\n",
    "        page_texts = []\n",
    "        for a in annual_report_pages:\n",
    "            if a.url.endswith(\".txt\"):\n",
    "                text = a.text\n",
    "            else:\n",
    "                asoup = BeautifulSoup(a.content, \"html.parser\")\n",
    "                ps = asoup.findAll(\"p\")\n",
    "                text = \"\\n\".join([p.text for p in ps])\n",
    "            page_texts.append(text)\n",
    "        report_pages[cik].extend(page_texts)\n",
    "\n",
    "        for i, page in enumerate(annual_report_pages):\n",
    "            try:\n",
    "                date_str = driver.find_elements_by_xpath(\n",
    "                    '//a[@data-index=\"reportDate\"]'\n",
    "                )[i].text\n",
    "                date = datetime.fromisoformat(date_str)\n",
    "            except:\n",
    "                logging.info(f\"{url} Did not read date\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            tmp_dict = dict()\n",
    "            if page.url.endswith(\".txt\"):\n",
    "                for entry in entries:\n",
    "                    tmp_dict[entry] = find_value_in_table_txt(page.text, entry)\n",
    "            else:\n",
    "                values = find_values_in_table(soup, entries)\n",
    "                for entry, value in zip(entries, values):\n",
    "                    tmp_dict[entry] = value\n",
    "            if (tmp_dict.keys() - set(entries)) == set():\n",
    "                for entry, value in tmp_dict.items():\n",
    "                    data_dict[entry].append(value)\n",
    "                data_dict[\"CIK\"].append(cik)\n",
    "                data_dict[\"date_filled\"].append(date)\n",
    "                data_dict[\"url\"].append(page.url)\n",
    "            else:\n",
    "                logging.info(f\"{page.url} Did not read html\")\n",
    "\n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    data_df.to_csv(\"../data/intermediate/scraped_financials.csv\")\n",
    "\n",
    "    with open(\"../data/intermediate/report_urls.pickle\", \"wb\") as f:\n",
    "        pickle.dump(report_urls, f)\n",
    "    d = {k: v for k, v in report_pages.items()}\n",
    "    with open(\"../data/intermediate/report_pages.pickle\", \"wb\") as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d696b-97d7-405c-819d-01afa9c5a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▊                                     | 12/69 [17:13<1:11:46, 75.56s/it]"
     ]
    }
   ],
   "source": [
    "run(ciks[436:]) # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218af31-d0e6-4ae0-b9e0-6d423a168368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "propulsion",
   "language": "python",
   "name": "propulsion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
